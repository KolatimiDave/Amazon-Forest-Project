{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Amazon_Planet.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOkuvxcfmev7u9giIAz4JgE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KolatimiDave/Amazon-Forest-Project/blob/master/Amazon_Planet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajbtDMn7gq35",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import Relevant Packages\n",
        "import os\n",
        "import cv2\n",
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.models import load_model\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop\n",
        "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.metrics import roc_curve, auc, roc_auc_score,fbeta_score\n",
        "from tensorflow.keras.applications import InceptionV3, VGG16, ResNet50, ResNet152\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten,Conv2D, GlobalAveragePooling2D, BatchNormalization\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38H_yAJvgwlQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define path and directory variables\n",
        "### Ensure to have downloaded the dataset from 'https://www.kaggle.com/nikitarom/planets-dataset'\n",
        "TRAIN_DIR = '/content/planet/train-jpg'\n",
        "TEST_DIR = '/content/planet/test-jpg'\n",
        "TEST_DIR_1 = '/content/test-jpg-additional'\n",
        "\n",
        "train_csv = pd.read_csv('/content/planet/train_classes.csv')\n",
        "sample = pd.read_csv('/content/planet/sample_submission.csv')\n",
        "train_path = '/content/planet/train-jpg/'\n",
        "\n",
        "print('GPU AVAILABILITY',tf.config.list_physical_devices('GPU'))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DhxORQ7xgwuG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get Unique tags\n",
        "label_list = []\n",
        "for tag_str in train_csv.tags.values:\n",
        "    labels = tag_str.split(' ')\n",
        "    for label in labels:\n",
        "        if label not in label_list:\n",
        "            label_list.append(label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DAkRHcIzgw9W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Add onehot features for every label\n",
        "for label in label_list:\n",
        "    train_csv[label] = train_csv['tags'].apply(lambda x: 1 if label in x.split(' ') else 0)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iGyfk395csaD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get test_id and image_path\n",
        "# First Test Directory\n",
        "image_name = []\n",
        "image_path = []\n",
        "for img in tqdm(os.listdir(TEST_DIR)):\n",
        "  image_name.append(img.split('.')[0])\n",
        "  image_path.append(TEST_DIR + '/' + img)\n",
        "\n",
        "# Second Test Directory\n",
        "image_name_1 = []\n",
        "image_path_1 = []\n",
        "for img in tqdm(os.listdir(TEST_DIR_1)):\n",
        "  image_name_1.append(img.split('.')[0])\n",
        "  image_path_1.append(TEST_DIR_1 + '/' + img)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v-cNvcSUcsWO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# combine the two\n",
        "image_name.extend(image_name_1) # Name of all Test images\n",
        "image_path.extend(image_path_1) # Path of all Test Images\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ih1gkOJ6csTi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#create dataframes and use keras Image data preprocessing function: flow_from_dataframe\n",
        "# Train\n",
        "train_data = train_csv.drop('tags',1)\n",
        "train_names = train_data.image_name.values\n",
        "train_data['filepath'] = train_path + train_names + '.jpg'\n",
        "train = train_data.copy()\n",
        "\n",
        "# Test\n",
        "test_names = pd.Series(image_name).values\n",
        "test_dict = {'filepath': image_path, 'image_name':test_names}\n",
        "test = pd.DataFrame(data=test_dict)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FajuhYU-csPE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('train and test shapes',train.shape, test.shape)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F_mKem3OgxOl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Splitting Train data into 2, the other is for validation.\n",
        "df_train = train[:36500:]\n",
        "df_val = train[36500:]\n",
        "print('train and validation shapes',df_train.shape, df_val.shape)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zuuCEQlvdPNC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#seed for reproducibility\n",
        "SEED = 20\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "tf.random.set_seed(SEED)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81_W7-eXdPeI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#data generators\n",
        "batch_size = 128\n",
        "image_size = (224, 224)\n",
        "learning_rate = 0.0001 \n",
        "\n",
        "train_steps = np.ceil(len(df_train) / batch_size)\n",
        "val_steps = np.ceil(len(df_val) / batch_size )\n",
        "\n",
        "classes = [i for i in train.columns.to_list() if i not in ['tags', 'image_name', 'filepath'] ]\n",
        "\n",
        "train_datagen = ImageDataGenerator(preprocessing_function=preprocess_input,horizontal_flip=True,\n",
        "                             vertical_flip=True,shear_range=10,zoom_range=0.2,width_shift_range=0.1,\n",
        "                             height_shift_range=0.1,channel_shift_range=10.)\n",
        "\n",
        "val_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
        "\n",
        "train_ds = train_datagen.flow_from_dataframe(df_train,x_col='filepath', y_col=classes,\n",
        "                                        target_size=image_size,classes=classes,\n",
        "                                        batch_size=batch_size,\n",
        "                                        class_mode='raw', shuffle=True, seed=SEED)\n",
        "\n",
        "val_ds = val_datagen.flow_from_dataframe(df_val,x_col='filepath', y_col=classes,\n",
        "                                        target_size=image_size,classes=classes,\n",
        "                                        batch_size=batch_size,\n",
        "                                        class_mode='raw', shuffle=False, seed=SEED)\n",
        "\n",
        "test_ds = val_datagen.flow_from_dataframe(test, x_col='filepath',target_size=image_size, class_mode=None,\n",
        "                                          shuffle=False,batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fkODALcldPay",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_valid_df = df_val.drop(columns=['image_name','filepath']) \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ek6qtP7idPWM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Evaluate Model\n",
        "def ModelEvaluator(model_eval_preds, h5_name, threshold=0.5):\n",
        "    score = fbeta_score(y_valid_df, np.array(model_eval_preds) > threshold, beta=2, average='samples')  \n",
        "    print('{} evaluation on validation_data with a threshold of {} = {}'.format(h5_name,threshold, score))\n",
        "    return score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFeDmvyKdPSr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Model Submission Files Generation\n",
        "\n",
        "def PredictionsClipper(preds, threshold=0.05):\n",
        "    ''' Changing our predictions into a DataFrame'''\n",
        "    preds = pd.DataFrame(preds, columns=y_valid_df.columns.to_list())\n",
        "    \n",
        "    new_features, tmp = [], []\n",
        "    for idx in range(preds.shape[0]):\n",
        "      for col in preds.columns.to_list():\n",
        "        if preds[col].iloc[idx]>threshold:\n",
        "          tmp.append(col)\n",
        "      new_features.append(tmp)\n",
        "      tmp = [] # Reseting the state of temporary[tmp] list back to being empty for every index[idx]\n",
        "\n",
        "    return new_features\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BQWhYj3EdPKM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def PredictionsFormater(pred_features):\n",
        "  ''' Changing our predicted tags to the submission format'''\n",
        "    tmp = ''\n",
        "    new_cols = []\n",
        "    for idx,_ in enumerate(pred_features):\n",
        "        for j in pred_features[idx]:\n",
        "            tmp = tmp + j+ ' ' \n",
        "        new = tmp[:-1]\n",
        "        new_cols.append(new)\n",
        "        tmp = '' # Reseting the state of temporary[tmp] string back to empty for every index[idx]\n",
        "\n",
        "    return new_cols"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JoUT-TkUdPHi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def PredictionsToCsv(new_cols, sub_name):\n",
        "    sub_dict = {'image_name': test.image_name, 'tags': pd.Series(new_cols)}\n",
        "    sub = pd.DataFrame(sub_dict)\n",
        "    sub.to_csv(sub_name, index=False)\n",
        "    \n",
        "    return sub\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2VsJGdVpdPE-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Network:\n",
        "    \n",
        "    def __init__(self): # Initialize class variables\n",
        "        self.model = None\n",
        "        self.preds = None\n",
        "        self.history = None\n",
        "        self.score = None\n",
        "        self.h5_name = None\n",
        "        self.model_eval_preds = None\n",
        "        self.threshold = None\n",
        "        self.preds = None\n",
        "        self.new_features = None\n",
        "        self.new_cols = None\n",
        "        \n",
        "    def BuildModel (self, pretrained):\n",
        "      ''' Transfer Learning '''\n",
        "        base_model = pretrained(include_top=False, weights='imagenet', input_shape=(224, 224,3)) #include_top=False to remove the last layer \n",
        "\n",
        "        for layer in base_model.layers:\n",
        "            layer.trainable = False   #freeze trainable layers\n",
        "\n",
        "        x = base_model.output\n",
        "        x = GlobalAveragePooling2D()(x)\n",
        "        x = Dropout(0.1)(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = Dense(3064, activation='relu')(x)\n",
        "        x = Dropout(0.3)(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        output = Dense(17, activation='sigmoid')(x) \n",
        "\n",
        "        self.model = Model(base_model.input, output)\n",
        "\n",
        "        return self.model\n",
        "    \n",
        "    def TrainModel(self, h5_name, nb_epochs = 30, patience = 5):\n",
        "        self.h5_name = h5_name\n",
        "        optimizer = keras.optimizers.Adam(lr=learning_rate)\n",
        "        self.model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['AUC'])\n",
        "\n",
        "        earlystop = EarlyStopping(monitor='val_loss', patience=patience, verbose=1, restore_best_weights=True)\n",
        "        chkpt_path = os.path.join(\"./\", self.h5_name+'_Amazon_Forest.h5', )\n",
        "        checkpoint = ModelCheckpoint(chkpt_path, monitor='val_loss',mode='auto', verbose=1, save_best_only=True)\n",
        "\n",
        "        self.history = self.model.fit(train_ds, epochs=nb_epochs,steps_per_epoch=train_steps, callbacks=[earlystop, checkpoint],\n",
        "                            verbose=1, shuffle=False,validation_data=(val_ds), validation_steps= val_steps)\n",
        "\n",
        "        start = time.time() # time module to calculate inference time on test dataset\n",
        "        self.preds = self.model.predict(test_ds) \n",
        "        inference_time = time.time() - start\n",
        "        print('{} inference time = {:.2f} minutes'.format(h5_name, inference_time/60))\n",
        "        \n",
        "        self.model_eval_preds = self.model.predict(val_ds) # Making predictions on the validation dataset\n",
        "        print('first', type(self.model_eval_preds))\n",
        "        \n",
        "        return self.preds, self.model, self.history\n",
        "    \n",
        "    def EvalModel(self, threshold=0.5):\n",
        "        self.threshold = threshold\n",
        "        self.score = ModelEvaluator(self.model_eval_preds, self.h5_name, self.threshold)\n",
        "        \n",
        "        return self.score\n",
        "    \n",
        "    def ClipPredictions(self):\n",
        "        self.new_features = PredictionsClipper(self.preds, self.threshold) \n",
        "        \n",
        "        return self.new_features\n",
        "    \n",
        "    def FormatPredictions(self):\n",
        "        self.new_cols = PredictionsFormater(self.new_features)\n",
        "\n",
        "        return self.new_cols\n",
        "    \n",
        "    def ToCsv(self,sub_name):\n",
        "        sub = PredictionsToCsv(self.new_cols, sub_name)\n",
        "        \n",
        "        return sub\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eas6dYVoeQRB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_trained_model(h5_name):\n",
        "    try:\n",
        "      if h5_name in [i[0:len(h5_name)] for i in os.listdir('./')]:\n",
        "          model = load_model(h5_name+'_Amazon_Forest.h5')\n",
        "          return model\n",
        "      else:print('.h5 Path incorrect ')\n",
        "    except OSError:\n",
        "      print('unable to load .h5 file from specified path')\n",
        "\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7bU7Ec41eQOU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def MakeCSVs(pretrained, h5_name, sub_name, threshold=0.5):\n",
        "    network = Network()\n",
        "    model = network.BuildModel(pretrained)\n",
        "    preds, fitted_model, model_history = network.TrainModel(h5_name=h5_name, nb_epochs=30)\n",
        "    score = network.EvalModel(threshold = 0.25)\n",
        "    pred_features = network.ClipPredictions()\n",
        "    formated_features = network.FormatPredictions()\n",
        "    sub = network.ToCsv(sub_name)\n",
        "    \n",
        "    return model, score, preds, \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gj5K84wbeQL2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ResNet152 Transfer learning\n",
        "res152_trained, fbeta_res152, res152_preds = MakeCSVs(ResNet152,'res152_model','res152.csv')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oRmQQtpmeQJb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# VGG16 Transfer learning\n",
        "vgg16_trained, fbeta_vgg16, vgg16_preds = MakeCSVs(VGG16,'vgg16_model','vgg16.csv')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UpbXw8kieQHf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ResNet52 Transfer learning\n",
        "res50_trained, fbeta_res50, res50_preds = MakeCSVs(ResNet50,'res50_model','res50.csv')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5H_6CwX_eQFk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "blended_pred = (res152_preds * 0.4)  + (vgg16_preds*0.2) + (res50_preds*0.4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-KprxQICeP_T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "blended_features = PredictionsClipper(V_preds, threshold = 0.22)\n",
        "blended_columns = PredictionsFormater(blended_features)\n",
        "blended_sub = PredictionsToCsv(blended_columns, sub_name='Blended.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKtXvTq0nTuK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## ResNet152 LB score of 0.90984\n",
        "##  VGG16 lB score of 0.90280\n",
        "## ResNet52 LB score of 0.91107\n",
        "\n",
        "## Blended LB score of 0.91630"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}